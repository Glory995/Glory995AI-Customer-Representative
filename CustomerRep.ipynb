{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7abf28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import openai \n",
    "import os  \n",
    "\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2bdcc5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key: sk-or-v1-a ...\n",
      "Base URL: https://openrouter.ai/api/v1/\n"
     ]
    }
   ],
   "source": [
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "    base_url=os.getenv(\"OPENROUTER_API_BASE\")\n",
    ")\n",
    "\n",
    "print(\"API Key:\", client.api_key[:10], \"...\")\n",
    "print(\"Base URL:\", client.base_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "126e70ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! Iâ€™m an artificial intelligence created by OpenAI, designed to assist with a wide range of tasks. You can think of me as a conversational partner or a helpful toolâ€”I can answer questions, provide explanations, generate creative content, assist with problem-solving, and much more. My knowledge is based on a vast amount of information up until October 2023, and Iâ€™m here to make your life easier or more interesting. Let me know how I can help you today! ðŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"deepseek/deepseek-chat\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Hello AI, please introduce yourself\"}\n",
    "    ],\n",
    "    max_tokens=500\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f58be083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AI's Answer:\n",
      "Python is a high-level, interpreted, and general-purpose programming language known for its simplicity, readability, and versatility.\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"deepseek/deepseek-chat\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What is Python in one sentence?\"}],\n",
    "    max_tokens=500\n",
    "\n",
    ")\n",
    "\n",
    "ai_text = response.choices[0].message.content \n",
    "\n",
    "\n",
    "print(\"\\nAI's Answer:\")\n",
    "print(ai_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc557a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  The question used: 10 tokens\n",
      "  AI's response used: 23 tokens\n",
      "  Total tokens billed: 33 tokens\n",
      "\n",
      " Cost Breakdown for This Call:\n",
      "  Input cost:  $0.000008 (10 tokens)\n",
      "  Output cost: $0.000074 (23 tokens)\n",
      "  TOTAL COST:  $0.000082\n"
     ]
    }
   ],
   "source": [
    "input_tokens = response.usage.prompt_tokens  \n",
    "output_tokens = response.usage.completion_tokens  \n",
    "total_tokens = response.usage.total_tokens    \n",
    "\n",
    "\n",
    "print(f\"  The question used: {input_tokens} tokens\")\n",
    "print(f\"  AI's response used: {output_tokens} tokens\")\n",
    "print(f\"  Total tokens billed: {total_tokens} tokens\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "input_price_per_1k = 0.0008   \n",
    "output_price_per_1k = 0.0032  \n",
    "\n",
    "\n",
    "input_cost = (input_tokens / 1000) * input_price_per_1k\n",
    "output_cost = (output_tokens / 1000) * output_price_per_1k\n",
    "total_cost = input_cost + output_cost\n",
    "\n",
    "print(\"\\n Cost Breakdown for This Call:\")\n",
    "print(f\"  Input cost:  ${input_cost:.6f} ({input_tokens} tokens)\")\n",
    "print(f\"  Output cost: ${output_cost:.6f} ({output_tokens} tokens)\")\n",
    "print(f\"  TOTAL COST:  ${total_cost:.6f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99ea752",
   "metadata": {},
   "source": [
    "## LangChain\n",
    "\n",
    "10+ lines of boilerplate. Want to switch to Google? Rewrite everything! brahhhhhh thanks to langChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd2f2db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Client Calling : **\"Machine learning is a subset of artificial intelligence that enables systems to learn and improve...\n",
      "Langhain Response result : \"Machine learning is a subset of artificial intelligence that enables systems to automatically learn...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "def raw_openai_approach():\n",
    "    \n",
    "    import openai\n",
    "\n",
    "    client = openai.OpenAI(\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "    base_url=os.getenv(\"OPENROUTER_API_BASE\")\n",
    ")\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "    model=\"deepseek/deepseek-chat\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Explain machine learning in one sentence\"}],\n",
    "    max_tokens=500\n",
    "\n",
    ")\n",
    "\n",
    "    if response:\n",
    "        text = response.choices[0].message.content \n",
    "        print(f\"Normal Client Calling : {text[:100]}...\")\n",
    "        return text\n",
    "\n",
    "    return None\n",
    "\n",
    "def langchain_approach():\n",
    "    from langchain_openai import ChatOpenAI\n",
    "\n",
    "    \n",
    "    llm = ChatOpenAI(\n",
    "        model=\"deepseek/deepseek-chat\",                    \n",
    "        api_key=os.getenv(\"OPENROUTER_API_KEY\"),      \n",
    "        base_url=os.getenv(\"OPENROUTER_API_BASE\"),      \n",
    "        max_tokens=500, \n",
    "    )\n",
    "\n",
    "    \n",
    "    response = llm.invoke(\"Explain machine learning in one sentence\")  \n",
    "\n",
    "    if response:\n",
    "        print(f\"Langhain Response result : {response.content[:100]}...\")\n",
    "        return response.content\n",
    "\n",
    "    return None\n",
    "\n",
    "def main():\n",
    "     \n",
    "    raw_openai_approach()\n",
    "    langchain_approach()\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b77c2219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending to AI: Explain artificial intelligence in exactly 5 words\n",
      "\n",
      " AI Response: Machines simulating human cognitive functions.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "def main():\n",
    "    \n",
    "    \n",
    "    template = PromptTemplate(\n",
    "        input_variables=[\"topic\", \"style\"], \n",
    "        template=\"Explain {topic} in {style}\" \n",
    "    )\n",
    "    llm = ChatOpenAI(\n",
    "        model=\"deepseek/deepseek-chat\",\n",
    "        api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "        base_url=os.getenv(\"OPENROUTER_API_BASE\"),\n",
    "        temperature=0.7,\n",
    "        max_tokens= 500\n",
    "    )\n",
    "\n",
    "    if template and llm:\n",
    "        \n",
    "        test_prompt = template.format(\n",
    "            topic=\"artificial intelligence\", \n",
    "            style=\"exactly 5 words\" \n",
    "        )\n",
    "\n",
    "        print(f\"Sending to AI: {test_prompt}\")\n",
    "        response = llm.invoke(test_prompt)\n",
    "        print(f\"\\n AI Response: {response.content}\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e723a9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Input: 'List 3 benefits of cloud computing'\n",
      "Parsed Output: ['Here are three benefits of cloud computing:  ', '1. **Cost Efficiency** (reduces upfront infrastructure costs)', '', '2. **Scalability** (easily adjust resources based on demand)', '', '3. **Accessibility** (access data and applications from anywhere).  ', \"Let me know if you'd like more details!\"]\n",
      "Type: <class 'list'>\n",
      "Access items: result[0] = 'Here are three benefits of cloud computing:  '\n",
      "Input: 'Analyze machine learning'\n",
      "   Benefits: ['Automates complex data processing tasks', 'Enhances decision-making with predictive analytics']\n",
      "   Complexity: high\n",
      "   Use Case: Predictive maintenance in manufacturing\n",
      "    Type: <class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "def main():\n",
    "    \n",
    "\n",
    "    llm = ChatOpenAI(\n",
    "        model=\"deepseek/deepseek-chat\",\n",
    "        api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "        base_url=os.getenv(\"OPENROUTER_API_BASE\"),\n",
    "        temperature=0.7,\n",
    "        max_tokens= 500\n",
    "    )   \n",
    "\n",
    "    list_parser = CommaSeparatedListOutputParser() \n",
    "\n",
    "  \n",
    "    list_prompt = PromptTemplate(\n",
    "        template=  \"List 3 benefits of {technology} (comma-separated):\",\n",
    "        input_variables=[\"technology\"]\n",
    "    )\n",
    "\n",
    "    list_chain = list_prompt | llm | list_parser \n",
    "   \n",
    "    if list_chain:\n",
    "        result = list_chain.invoke({\n",
    "            \"technology\": \"cloud computing\"\n",
    "        })\n",
    "        print(f\" Input: 'List 3 benefits of cloud computing'\")\n",
    "        print(f\"Parsed Output: {result}\")\n",
    "        print(f\"Type: {type(result)}\")\n",
    "        print(f\"Access items: result[0] = '{result[0] if result else ''}'\")\n",
    "\n",
    "    \n",
    "    \n",
    "    json_parser = JsonOutputParser()\n",
    "\n",
    "    json_prompt = PromptTemplate(\n",
    "        template=\"\"\"Analyze {technology} and respond with JSON containing:\n",
    "        - benefits: array of 2 benefits\n",
    "        - complexity: low/medium/high\n",
    "        - use_case: one main use case\n",
    "\n",
    "        Technology: {technology}\n",
    "\n",
    "        {format_instructions}\"\"\",\n",
    "        input_variables=[\"technology\"],\n",
    "        partial_variables={\"format_instructions\": json_parser.get_format_instructions()}\n",
    "    )\n",
    "\n",
    "    json_chain = json_prompt | llm | json_parser \n",
    "\n",
    "   \n",
    "    if json_chain:\n",
    "        result = json_chain.invoke({\n",
    "            \"technology\": \"machine learning\"\n",
    "        })\n",
    "\n",
    "        print(f\"Input: 'Analyze machine learning'\")\n",
    "\n",
    "        try:\n",
    "      \n",
    "            parsed = result \n",
    "\n",
    "            print(f\"   Benefits: {parsed.get('benefits', [])}\")\n",
    "            print(f\"   Complexity: {parsed.get('complexity', 'N/A')}\")\n",
    "            print(f\"   Use Case: {parsed.get('use_case', 'N/A')}\")\n",
    "            print(f\"    Type: {type(parsed)}\" )\n",
    "        except (json.JSONDecodeError, TypeError, AttributeError):\n",
    "            print(f\" Parsing failed : {result}\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e384c32c",
   "metadata": {},
   "source": [
    "## Complete Chain - Combining Everything!\n",
    "## Build complete AI pipelines using LangChain's chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "130b4cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Input: 'Analyze blockchain'\n",
      " Output: Blockchain offers **decentralization**, **transparency**, and **security**, making it ideal for applications like cryptocurrency, supply chain tracking, and smart contracts. However, its **scalability** is limited, transaction speeds can be slow, and energy consumption (e.g., Proof of Work) raises environmental concerns. Additionally, regulatory uncertainty and potential for misuse (e.g., illicit activities) remain challenges.\n",
      " Input: 'List use cases for blockchain'\n",
      " Output: ['Here are three common use cases for blockchain (comma-separated):  ', '1. **Cryptocurrency transactions (e.g.', 'Bitcoin', 'Ethereum)**', '', '2. **Supply chain tracking and transparency**', '', '3. **Smart contracts for automated agreements**.  ', \"Let me know if you'd like more details!\"]\n",
      "âœ… Type: <class 'list'> - Python list!\n",
      "Technology: artificial intelligence\n",
      "\n",
      " Analysis:\n",
      "   ### **Pros of Artificial Intelligence (AI):**  \n",
      "1. **Efficiency & Automation:** AI can process vast amounts of data quickly, automate repetitive tasks, and improve productivity in industries like healthcare, finance, and manufacturing.  \n",
      "2. **Enhanced Decision-Making:** AI-powered analytics help businesses and researchers make data-driven decisions with greater accuracy and speed.  \n",
      "3. **Innovation & Convenience:** AI enables breakthroughs in areas like self-driving cars, virtual assistants, and personalized recommendations, improving daily life.  \n",
      "\n",
      "### **Cons of Artificial Intelligence (AI):**  \n",
      "1. **Job Displacement:** Automation may replace human jobs, particularly in manual and repetitive roles, leading to unemployment concerns.  \n",
      "2. **Bias & Ethical Issues:** AI systems can inherit biases from training data, leading to unfair or discriminatory outcomes in hiring, law enforcement, and lending.  \n",
      "3. **Security & Privacy Risks:** AI can be exploited for cyberattacks, deepfakes, and surveillance, raising concerns about data misuse and loss of privacy.  \n",
      "\n",
      "Would you like a deeper analysis on any specific aspect?\n",
      "\n",
      " Use Cases:\n",
      "   1. Automated customer support\n",
      "   2. predictive analytics in healthcare\n",
      "   3. autonomous driving systems.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "def main():\n",
    "    \n",
    "\n",
    "    \n",
    "    llm = ChatOpenAI(\n",
    "        model=\"deepseek/deepseek-chat\",\n",
    "        api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "        base_url=os.getenv(\"OPENROUTER_API_BASE\"),\n",
    "        temperature=0.7,\n",
    "        max_tokens= 500\n",
    "    )  \n",
    "    \n",
    "    analysis_prompt = PromptTemplate(\n",
    "        template=\"Analyze {technology} and provide pros and cons in 2-3 sentences\",\n",
    "        input_variables=[\"technology\"]\n",
    "    )\n",
    "\n",
    "    str_parser = StrOutputParser()\n",
    "\n",
    "    \n",
    "    analysis_chain = analysis_prompt | llm | str_parser \n",
    "\n",
    "    \n",
    "    if analysis_chain:\n",
    "        result = analysis_chain.invoke({\n",
    "            \"technology\": \"blockchain\"\n",
    "        })\n",
    "        print(f\" Input: 'Analyze blockchain'\")\n",
    "        print(f\" Output: {result}\")\n",
    "\n",
    "\n",
    "   \n",
    "    list_prompt = PromptTemplate(\n",
    "        template=\"List 3 use cases for {technology} (comma-separated):\",\n",
    "        input_variables=[\"technology\"]\n",
    "    )\n",
    "\n",
    "    \n",
    "    list_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "    \n",
    "    list_chain = list_prompt | llm | list_parser \n",
    "\n",
    "    if list_chain:\n",
    "        result = list_chain.invoke({\n",
    "            \"technology\": \"blockchain\"\n",
    "        })\n",
    "        print(f\" Input: 'List use cases for blockchain'\")\n",
    "        print(f\" Output: {result}\")\n",
    "        print(f\" Type: {type(result)} - Python list!\")\n",
    "\n",
    "    \n",
    "    test_tech = \"artificial intelligence\"\n",
    "    print(f\"Technology: {test_tech}\\n\")\n",
    "\n",
    "    \n",
    "    if analysis_chain and list_chain:\n",
    "        analysis = analysis_chain.invoke({\"technology\": test_tech})\n",
    "        print(f\" Analysis:\\n   {analysis}\")\n",
    "\n",
    "        use_cases = list_chain.invoke({\"technology\": test_tech})\n",
    "        print(f\"\\n Use Cases:\")\n",
    "        for i, use_case in enumerate(use_cases, 1):\n",
    "            print(f\"   {i}. {use_case}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f895f800",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
