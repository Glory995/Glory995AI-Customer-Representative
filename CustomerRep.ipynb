{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7abf28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import openai \n",
    "import os  \n",
    "\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2bdcc5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key: sk-or-v1-f ...\n",
      "Base URL: https://openrouter.ai/api/v1/\n"
     ]
    }
   ],
   "source": [
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "    base_url=os.getenv(\"OPENROUTER_API_BASE\")\n",
    ")\n",
    "\n",
    "print(\"API Key:\", client.api_key[:10], \"...\")\n",
    "print(\"Base URL:\", client.base_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "126e70ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! Iâ€™m an AI language model created by OpenAI, and Iâ€™m here to assist with answering questions, providing explanations, generating ideas, or just having a conversation about almost any topic. I donâ€™t have personal experiences or consciousness, but Iâ€™m trained on a vast amount of information to help with a wide range of tasks. Let me know how I can assist you today! ðŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"deepseek/deepseek-chat\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Hello AI, please introduce yourself\"}\n",
    "    ],\n",
    "    max_tokens=500\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f58be083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AI's Answer:\n",
      "Python is a high-level, versatile programming language known for its simplicity, readability, and extensive libraries, making it suitable for a wide range of applications from web development to data science and artificial intelligence.\n"
     ]
    }
   ],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"deepseek/deepseek-chat\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What is Python in one sentence?\"}],\n",
    "    max_tokens=500\n",
    "\n",
    ")\n",
    "\n",
    "ai_text = response.choices[0].message.content \n",
    "\n",
    "\n",
    "print(\"\\nAI's Answer:\")\n",
    "print(ai_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc557a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  The question used: 10 tokens\n",
      "  AI's response used: 40 tokens\n",
      "  Total tokens billed: 50 tokens\n",
      "\n",
      " Cost Breakdown for This Call:\n",
      "  Input cost:  $0.000008 (10 tokens)\n",
      "  Output cost: $0.000128 (40 tokens)\n",
      "  TOTAL COST:  $0.000136\n"
     ]
    }
   ],
   "source": [
    "input_tokens = response.usage.prompt_tokens  \n",
    "output_tokens = response.usage.completion_tokens  \n",
    "total_tokens = response.usage.total_tokens    \n",
    "\n",
    "\n",
    "print(f\"  The question used: {input_tokens} tokens\")\n",
    "print(f\"  AI's response used: {output_tokens} tokens\")\n",
    "print(f\"  Total tokens billed: {total_tokens} tokens\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "input_price_per_1k = 0.0008   \n",
    "output_price_per_1k = 0.0032  \n",
    "\n",
    "\n",
    "input_cost = (input_tokens / 1000) * input_price_per_1k\n",
    "output_cost = (output_tokens / 1000) * output_price_per_1k\n",
    "total_cost = input_cost + output_cost\n",
    "\n",
    "print(\"\\n Cost Breakdown for This Call:\")\n",
    "print(f\"  Input cost:  ${input_cost:.6f} ({input_tokens} tokens)\")\n",
    "print(f\"  Output cost: ${output_cost:.6f} ({output_tokens} tokens)\")\n",
    "print(f\"  TOTAL COST:  ${total_cost:.6f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99ea752",
   "metadata": {},
   "source": [
    "## LangChain\n",
    "\n",
    "10+ lines of boilerplate. Want to switch to Google? Rewrite everything! brahhhhhh thanks to langChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd2f2db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Client Calling : Machine learning is a subset of artificial intelligence that enables systems to learn and improve fr...\n",
      "Langhain Response result : Machine learning is the process of training algorithms to learn patterns and make predictions or dec...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "def raw_openai_approach():\n",
    "    \n",
    "    import openai\n",
    "\n",
    "    client = openai.OpenAI(\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "    base_url=os.getenv(\"OPENROUTER_API_BASE\")\n",
    ")\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "    model=\"deepseek/deepseek-chat\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Explain machine learning in one sentence\"}],\n",
    "    max_tokens=500\n",
    "\n",
    ")\n",
    "\n",
    "    if response:\n",
    "        text = response.choices[0].message.content \n",
    "        print(f\"Normal Client Calling : {text[:100]}...\")\n",
    "        return text\n",
    "\n",
    "    return None\n",
    "\n",
    "def langchain_approach():\n",
    "    from langchain_openai import ChatOpenAI\n",
    "\n",
    "    \n",
    "    llm = ChatOpenAI(\n",
    "        model=\"deepseek/deepseek-chat\",                    \n",
    "        api_key=os.getenv(\"OPENROUTER_API_KEY\"),      \n",
    "        base_url=os.getenv(\"OPENROUTER_API_BASE\"),      \n",
    "        max_tokens=500, \n",
    "    )\n",
    "\n",
    "    \n",
    "    response = llm.invoke(\"Explain machine learning in one sentence\")  \n",
    "\n",
    "    if response:\n",
    "        print(f\"Langhain Response result : {response.content[:100]}...\")\n",
    "        return response.content\n",
    "\n",
    "    return None\n",
    "\n",
    "def main():\n",
    "     \n",
    "    raw_openai_approach()\n",
    "    langchain_approach()\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b77c2219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending to AI: Explain artificial intelligence in exactly 5 words\n",
      "\n",
      " AI Response: Machines simulating human-like intelligence processes.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "def main():\n",
    "    \n",
    "    \n",
    "    template = PromptTemplate(\n",
    "        input_variables=[\"topic\", \"style\"], \n",
    "        template=\"Explain {topic} in {style}\" \n",
    "    )\n",
    "    llm = ChatOpenAI(\n",
    "        model=\"deepseek/deepseek-chat\",\n",
    "        api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "        base_url=os.getenv(\"OPENROUTER_API_BASE\"),\n",
    "        temperature=0.7,\n",
    "        max_tokens= 500\n",
    "    )\n",
    "\n",
    "    if template and llm:\n",
    "        \n",
    "        test_prompt = template.format(\n",
    "            topic=\"artificial intelligence\", \n",
    "            style=\"exactly 5 words\" \n",
    "        )\n",
    "\n",
    "        print(f\"Sending to AI: {test_prompt}\")\n",
    "        response = llm.invoke(test_prompt)\n",
    "        print(f\"\\n AI Response: {response.content}\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e723a9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Input: 'List 3 benefits of cloud computing'\n",
      "Parsed Output: ['Cost efficiency', 'scalability', 'accessibility.']\n",
      "Type: <class 'list'>\n",
      "Access items: result[0] = 'Cost efficiency'\n",
      "Input: 'Analyze machine learning'\n",
      "   Benefits: ['Automates complex decision-making processes', 'Enhances predictive accuracy based on historical data']\n",
      "   Complexity: high\n",
      "   Use Case: Predictive analytics in finance\n",
      "    Type: <class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n",
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "def main():\n",
    "    \n",
    "\n",
    "    llm = ChatOpenAI(\n",
    "        model=\"deepseek/deepseek-chat\",\n",
    "        api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "        base_url=os.getenv(\"OPENROUTER_API_BASE\"),\n",
    "        temperature=0.7,\n",
    "        max_tokens= 500\n",
    "    )\n",
    "       \n",
    "\n",
    "    list_parser = CommaSeparatedListOutputParser() \n",
    "\n",
    "  \n",
    "    list_prompt = PromptTemplate(\n",
    "        template=  \"List 3 benefits of {technology} (comma-separated):\",\n",
    "        input_variables=[\"technology\"]\n",
    "    )\n",
    "\n",
    "    list_chain = list_prompt | llm | list_parser \n",
    "   \n",
    "    if list_chain:\n",
    "        result = list_chain.invoke({\n",
    "            \"technology\": \"cloud computing\"\n",
    "        })\n",
    "        print(f\" Input: 'List 3 benefits of cloud computing'\")\n",
    "        print(f\"Parsed Output: {result}\")\n",
    "        print(f\"Type: {type(result)}\")\n",
    "        print(f\"Access items: result[0] = '{result[0] if result else ''}'\")\n",
    "\n",
    "    \n",
    "    \n",
    "    json_parser = JsonOutputParser()\n",
    "\n",
    "    json_prompt = PromptTemplate(\n",
    "        template=\"\"\"Analyze {technology} and respond with JSON containing:\n",
    "        - benefits: array of 2 benefits\n",
    "        - complexity: low/medium/high\n",
    "        - use_case: one main use case\n",
    "\n",
    "        Technology: {technology}\n",
    "\n",
    "        {format_instructions}\"\"\",\n",
    "        input_variables=[\"technology\"],\n",
    "        partial_variables={\"format_instructions\": json_parser.get_format_instructions()}\n",
    "    )\n",
    "\n",
    "    json_chain = json_prompt | llm | json_parser \n",
    "\n",
    "   \n",
    "    if json_chain:\n",
    "        result = json_chain.invoke({\n",
    "            \"technology\": \"machine learning\"\n",
    "        })\n",
    "\n",
    "        print(f\"Input: 'Analyze machine learning'\")\n",
    "\n",
    "        try:\n",
    "      \n",
    "            parsed = result \n",
    "\n",
    "            print(f\"   Benefits: {parsed.get('benefits', [])}\")\n",
    "            print(f\"   Complexity: {parsed.get('complexity', 'N/A')}\")\n",
    "            print(f\"   Use Case: {parsed.get('use_case', 'N/A')}\")\n",
    "            print(f\"    Type: {type(parsed)}\" )\n",
    "        except (json.JSONDecodeError, TypeError, AttributeError):\n",
    "            print(f\" Parsing failed : {result}\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e384c32c",
   "metadata": {},
   "source": [
    "## Complete Chain - Combining Everything!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "130b4cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Input: 'Analyze blockchain'\n",
      " Output: **Pros:** Blockchain offers enhanced security through decentralization and cryptographic hashing, ensuring data integrity and reducing fraud. It also enables transparent, tamper-proof records and eliminates intermediaries, reducing costs and increasing efficiency.  \n",
      "\n",
      "**Cons:** Blockchain can be resource-intensive, requiring significant energy consumption for mining and maintenance. Scalability issues and slower transaction speeds compared to traditional systems can limit its widespread adoption.\n",
      " Input: 'List use cases for blockchain'\n",
      " Output: ['Here are three common use cases for blockchain technology', 'separated by commas:  ', '1. **Cryptocurrencies (e.g.', 'Bitcoin', 'Ethereum)**', '', '2. **Supply Chain Tracking & Transparency**', '', '3. **Smart Contracts & Decentralized Applications (DApps)**  ', \"Let me know if you'd like more details or additional examples!\"]\n",
      " Type: <class 'list'> - Python list!\n",
      "Technology: artificial intelligence\n",
      "\n",
      " Analysis:\n",
      "   ### **Pros of Artificial Intelligence (AI):**  \n",
      "1. **Efficiency & Automation:** AI can process vast amounts of data quickly, automate repetitive tasks, and improve productivity in industries like healthcare, finance, and manufacturing.  \n",
      "2. **Enhanced Decision-Making:** AI-powered analytics help businesses and researchers make data-driven decisions, reducing human error and bias.  \n",
      "\n",
      "### **Cons of Artificial Intelligence (AI):**  \n",
      "1. **Job Displacement:** Automation may replace certain jobs, leading to unemployment and economic inequality in some sectors.  \n",
      "2. **Ethical & Privacy Concerns:** AI raises issues like data misuse, lack of transparency in decision-making (e.g., \"black box\" algorithms), and potential biases in training data.  \n",
      "\n",
      "Would you like a deeper analysis on any specific aspect?\n",
      "\n",
      " Use Cases:\n",
      "   1. Here are three use cases for artificial intelligence (comma-separated):  \n",
      "   2. **1. Personalized recommendations (e.g.\n",
      "   3. Netflix\n",
      "   4. Spotify)\n",
      "   5. 2. Autonomous vehicles (e.g.\n",
      "   6. self-driving cars)\n",
      "   7. 3. Medical diagnosis (e.g.\n",
      "   8. AI-assisted radiology)**  \n",
      "   9. Let me know if you'd like more examples!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "def main():\n",
    "    \n",
    "\n",
    "    \n",
    "    llm = ChatOpenAI(\n",
    "        model=\"deepseek/deepseek-chat\",\n",
    "        api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "        base_url=os.getenv(\"OPENROUTER_API_BASE\"),\n",
    "        temperature=0.7,\n",
    "        max_tokens= 500\n",
    "    )  \n",
    "    \n",
    "    analysis_prompt = PromptTemplate(\n",
    "        template=\"Analyze {technology} and provide pros and cons in 2-3 sentences\",\n",
    "        input_variables=[\"technology\"]\n",
    "    )\n",
    "\n",
    "    str_parser = StrOutputParser()\n",
    "\n",
    "    \n",
    "    analysis_chain = analysis_prompt | llm | str_parser \n",
    "\n",
    "    \n",
    "    if analysis_chain:\n",
    "        result = analysis_chain.invoke({\n",
    "            \"technology\": \"blockchain\"\n",
    "        })\n",
    "        print(f\" Input: 'Analyze blockchain'\")\n",
    "        print(f\" Output: {result}\")\n",
    "\n",
    "\n",
    "   \n",
    "    list_prompt = PromptTemplate(\n",
    "        template=\"List 3 use cases for {technology} (comma-separated):\",\n",
    "        input_variables=[\"technology\"]\n",
    "    )\n",
    "\n",
    "    \n",
    "    list_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "    \n",
    "    list_chain = list_prompt | llm | list_parser \n",
    "\n",
    "    if list_chain:\n",
    "        result = list_chain.invoke({\n",
    "            \"technology\": \"blockchain\"\n",
    "        })\n",
    "        print(f\" Input: 'List use cases for blockchain'\")\n",
    "        print(f\" Output: {result}\")\n",
    "        print(f\" Type: {type(result)} - Python list!\")\n",
    "\n",
    "    \n",
    "    test_tech = \"artificial intelligence\"\n",
    "    print(f\"Technology: {test_tech}\\n\")\n",
    "\n",
    "    \n",
    "    if analysis_chain and list_chain:\n",
    "        analysis = analysis_chain.invoke({\"technology\": test_tech})\n",
    "        print(f\" Analysis:\\n   {analysis}\")\n",
    "\n",
    "        use_cases = list_chain.invoke({\"technology\": test_tech})\n",
    "        print(f\"\\n Use Cases:\")\n",
    "        for i, use_case in enumerate(use_cases, 1):\n",
    "            print(f\"   {i}. {use_case}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a07e612",
   "metadata": {},
   "source": [
    "## Prompt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f895f800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write a policy\n",
      "\n",
      " response preview: Certainly! Below is a template for a **Workplace Remote Work Policy** that you can customize based o...\n",
      " Specific prompt: Write a 200-word GDPR-compliant data privacy polic\n",
      "\n",
      "Specific response : **GDPR-Compliant Data Privacy Policy**  \n",
      "\n",
      "At [Your Company Name], we are committed to protecting the privacy and security of our European customersâ€™ personal data in accordance with the General Data P...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "def main():\n",
    "    \n",
    "    llm = ChatOpenAI(\n",
    "        model=\"deepseek/deepseek-chat\",\n",
    "        api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "        base_url=os.getenv(\"OPENROUTER_API_BASE\"),\n",
    "        temperature=0.7,\n",
    "        max_tokens= 500\n",
    "    )  \n",
    "\n",
    "    \n",
    "    vague_prompt = \"write a policy\" \n",
    "\n",
    "    print(f\"{vague_prompt}\")\n",
    "\n",
    "    \n",
    "    vague_response = llm.invoke(vague_prompt)\n",
    "    print(f\"\\n response preview: {vague_response.content[:100]}...\")\n",
    "    \n",
    "    specific_prompt = \"\"\"Write a 200-word GDPR-compliant data privacy policy\n",
    "         for European customers with 30-day retention period\"\"\"\n",
    "\n",
    "    print(f\" Specific prompt: {specific_prompt[:50]}\")\n",
    "\n",
    "    \n",
    "    specific_response = llm.invoke(specific_prompt)\n",
    "    print(f\"\\nSpecific response : {specific_response.content[:200]}...\")\n",
    "    \n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aea8b28e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"REFUND POLICY\n",
      "1. Eligibility: Within 30 days of purchase\n",
      "2. Conditions: Product unused and in original packaging\n",
      "3. Process: Submit request via support@company.com\n",
      "4. Timeline: Refund processed within 5-7 business days\n",
      "5. Exceptions: Digital products and custom orders non-refundable\n",
      "content='\"REMOTE WORK POLICY  \\n1. Eligibility: Available to full-time employees in eligible roles  \\n2. Conditions: Must maintain productivity and meet performance expectations  \\n3. Process: Submit remote work request via HR@company.com for approval  \\n4. Timeline: Approval or denial communicated within 3-5 business days  \\n5. Exceptions: Roles requiring on-site presence or specific equipment are ineligible\"' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 81, 'prompt_tokens': 98, 'total_tokens': 179, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}, 'cost': 0.00010345, 'is_byok': False, 'cost_details': {'upstream_inference_cost': 0.00010345, 'upstream_inference_prompt_cost': 3.136e-05, 'upstream_inference_completions_cost': 7.209e-05}}, 'model_provider': 'openai', 'model_name': 'deepseek/deepseek-chat', 'system_fingerprint': None, 'id': 'gen-1769527102-DwtiPXvNBdTvDRdHfJNz', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--019bffd0-92e8-7f51-b381-37ed9ce38d4f-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 98, 'output_tokens': 81, 'total_tokens': 179, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      " follows the exact format of our example\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "#oneshot\n",
    "def main():\n",
    "    \n",
    "    llm = ChatOpenAI(\n",
    "        model=\"deepseek/deepseek-chat\",\n",
    "        api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "        base_url=os.getenv(\"OPENROUTER_API_BASE\"),\n",
    "        temperature=0.7,\n",
    "        max_tokens= 500\n",
    "    ) \n",
    "\n",
    "    example_policy = \"\"\"\"REFUND POLICY\\n1. Eligibility: Within 30 days of purchase\\n2. Conditions: Product unused and in original packaging\\n3. Process: Submit request via support@company.com\\n4. Timeline: Refund processed within 5-7 business days\\n5. Exceptions: Digital products and custom orders non-refundable\"\"\"  # Replace ___ with: \"REFUND POLICY\\n1. Eligibility: Within 30 days of purchase\\n2. Conditions: Product unused and in original packaging\\n3. Process: Submit request via support@company.com\\n4. Timeline: Refund processed within 5-7 business days\\n5. Exceptions: Digital products and custom orders non-refundable\"\n",
    "\n",
    "    \n",
    "    print(example_policy)\n",
    "\n",
    "    \n",
    "    one_shot_template = PromptTemplate(\n",
    "        template=\"\"\"Here's an example of our policy format:\n",
    "\n",
    "        {example}\n",
    "\n",
    "        Now write a {policy_type} policy following this EXACT format with numbered sections:\"\"\",\n",
    "        input_variables=[\"example\", \"policy_type\"] \n",
    "    )\n",
    "\n",
    "    \n",
    "\n",
    "    formatted_prompt = one_shot_template.format(\n",
    "        example=example_policy,\n",
    "        policy_type=\"remote work\"  \n",
    "    )\n",
    "\n",
    "    \n",
    "    response = llm.invoke(formatted_prompt)\n",
    "\n",
    "    print(response)\n",
    "    \n",
    "    has_numbered_sections = any(f\"{i}.\" in response.content for i in range(1, 6))\n",
    "    has_consistent_structure = all(\n",
    "        keyword in response.content.lower()\n",
    "        for keyword in [\"eligibility\", \"conditions\", \"process\", \"timeline\"]\n",
    "    )\n",
    "\n",
    "    if has_numbered_sections and has_consistent_structure:\n",
    "        print(\" follows the exact format of our example\")\n",
    "        \n",
    "    else:\n",
    "        print(\"Format needs adjustment\")\n",
    "\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b3adb0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples loaded:\n",
      "  Example 1: refund request â†’ I understand you'd like a refund. Let me check you...\n",
      "  Example 2: password reset â†’ I'll help you reset your password. For security, I...\n",
      " New customer issue: account locked\n",
      "\n",
      " AI Response: Iâ€™m sorry to hear your account is locked. For security reasons, Iâ€™ll need to verify some details to unlock it. Could you please confirm the email or phone number associated with your account? Once verified, I can assist you right away.  \n",
      "\n",
      "Let me know if youâ€™d prefer another verification method, and Iâ€™d be happy to help!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "\n",
    "def main():\n",
    "    \n",
    "    llm = ChatOpenAI(\n",
    "        model=\"deepseek/deepseek-chat\",\n",
    "        api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "        base_url=os.getenv(\"OPENROUTER_API_BASE\"),\n",
    "        temperature=0.7,\n",
    "        max_tokens= 500\n",
    "    ) \n",
    "\n",
    "    \n",
    "    examples = [\n",
    "        {\n",
    "            \"input\": \"refund request\",  \n",
    "            \"output\": \"I understand you'd like a refund. Let me check your order details. Our refund policy allows returns within 30 days. I'll process this for you right away.\"  # Replace ___ with: \"I understand you'd like a refund. Let me check your order details. Our refund policy allows returns within 30 days. I'll process this for you right away.\"\n",
    "        },\n",
    "        {\n",
    "            \"input\": \"shipping delay\",  \n",
    "            \"output\": \"I apologize for the shipping delay. Let me track your package immediately. I see it's currently in transit and should arrive within 2 days. I'll apply a shipping credit to your account.\",\n",
    "            \"input\": \"password reset\",\n",
    "            \"output\": \"I'll help you reset your password. For security, I've sent a reset link to your registered email. The link expires in 1 hour. Please check your spam folder if you don't see it.\"  \n",
    "        }\n",
    "    ]\n",
    "\n",
    "    print(\"Examples loaded:\")\n",
    "    for i, ex in enumerate(examples, 1):\n",
    "        print(f\"  Example {i}: {ex['input']} â†’ {ex['output'][:50]}...\")\n",
    "\n",
    "    \n",
    "    example_prompt = PromptTemplate(\n",
    "        template=\"Customer Issue: {input}\\nSupport Response: {output}\",\n",
    "        input_variables=[\"input\", \"output\"] \n",
    "    )\n",
    "\n",
    "    few_shot_prompt = FewShotPromptTemplate(\n",
    "        examples=examples,\n",
    "        example_prompt=example_prompt,\n",
    "        prefix=\"You are a helpful customer support agent. Here are examples of how to respond:\",\n",
    "        suffix=\"Customer Issue: {input}\\nSupport Response:\",\n",
    "        input_variables=[\"input\"]\n",
    "    )\n",
    "\n",
    "   \n",
    "\n",
    "    test_input = \"account locked\"  \n",
    "\n",
    "    formatted_prompt = few_shot_prompt.format(input=test_input)\n",
    "\n",
    "    print(f\" New customer issue: {test_input}\")\n",
    "    \n",
    "\n",
    "  \n",
    "    response = llm.invoke(formatted_prompt)\n",
    "    print(f\"\\n AI Response: {response.content}\")\n",
    "\n",
    "   \n",
    "\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "082f2538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fix our data retention policy to comply with GDPR.\n",
      "\n",
      "Direct response preview: To ensure your **data retention policy** complies with the **General Data Protection Regulation (GDPR)**, follow these key steps:\n",
      "\n",
      "### **1. Define Legal Basis for Data Retention**\n",
      "   - **Explicit Cons...\n",
      "\n",
      " Chain-of-Thought Approach\n",
      "Step 1: Review current GDPR requirements for data retention\n",
      "Step 2: Identify gaps in our existing policy\n",
      "Step 3: Research industry best practices\n",
      "Step 4: Draft specific recommendations\n",
      "Step 5: Create implementation timeline\n",
      "\n",
      " Chain-of-Thought Response:\n",
      "Letâ€™s systematically address each step to fix your data retention policy for GDPR compliance:\n",
      "\n",
      "---\n",
      "\n",
      "### **Step 1: Review Current GDPR Requirements for Data Retention**\n",
      "**Key GDPR Principles to Consider:**\n",
      "1. **Storage Limitation (Article 5(1)(e))**:  \n",
      "   - Personal data must be kept no longer than necessary for the purpose it was collected.  \n",
      "   - Retention periods must be justified and documented.  \n",
      "2. **Purpose Limitation (Article 5(1)(b))**:  \n",
      "   - Data should not be retained \"just in case\"; ...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "def main():\n",
    "    \n",
    "    llm = ChatOpenAI(\n",
    "        model=\"deepseek/deepseek-chat\",\n",
    "        api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "        base_url=os.getenv(\"OPENROUTER_API_BASE\"),\n",
    "        temperature=0.7,\n",
    "        max_tokens= 500\n",
    "    )\n",
    "\n",
    "    \n",
    "    direct_prompt = \"Fix our data retention policy to comply with GDPR.\"\n",
    "\n",
    "    print(f\"{direct_prompt}\")\n",
    "    direct_response = llm.invoke(direct_prompt)\n",
    "    print(f\"\\nDirect response preview: {direct_response.content[:200]}...\")\n",
    "\n",
    "\n",
    "    print(\"\\n Chain-of-Thought Approach\")\n",
    " \n",
    "    \n",
    "    reasoning_steps = \"\"\"Step 1: Review current GDPR requirements for data retention\\nStep 2: Identify gaps in our existing policy\\nStep 3: Research industry best practices\\nStep 4: Draft specific recommendations\\nStep 5: Create implementation timeline\"\"\"  # Replace ___ with: \"Step 1: Review current GDPR requirements for data retention\\nStep 2: Identify gaps in our existing policy\\nStep 3: Research industry best practices\\nStep 4: Draft specific recommendations\\nStep 5: Create implementation timeline\"\n",
    "\n",
    "    print(reasoning_steps)\n",
    "\n",
    "    cot_template = PromptTemplate(\n",
    "        template=\"\"\"To solve this problem, think through it step-by-step:\n",
    "\n",
    "            {steps}\n",
    "\n",
    "            Problem: {problem}\n",
    "\n",
    "            Now, let's work through each step systematically:\"\"\",\n",
    "            input_variables=[\"steps\", \"problem\"]  \n",
    "    )\n",
    "\n",
    "    \n",
    "    cot_prompt = cot_template.format(\n",
    "        steps=reasoning_steps,\n",
    "        problem=\"Fix our data retention policy to comply with GDPR\"  \n",
    "    )\n",
    "\n",
    "\n",
    "    cot_response = llm.invoke(cot_prompt)\n",
    "    print(f\"\\n Chain-of-Thought Response:\\n{cot_response.content[:500]}...\")\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d81878",
   "metadata": {},
   "source": [
    "## Lets put everything together "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "508191b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response length: 461 characters\n",
      "Preview: It seems like you've mentioned \"test_problem,\" but I haven't received enough context to understand w...\n",
      "\n",
      "Response length: 1461 characters\n",
      "Preview: Hereâ€™s a **test problem** based on the provided **Vacation Policy** example. This problem assesses u...\n",
      "\n",
      "Response length: 504 characters\n",
      "Preview: Hereâ€™s an example of a policy format for **test_problem**:\n",
      "\n",
      "- **test_problem**:  \n",
      "  1. **Identificat...\n",
      "\n",
      "Response length: 2224 characters\n",
      "Preview: Here's a step-by-step breakdown to create a remote work policy based on the given problem (`test_pro...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate, FewShotPromptTemplate\n",
    "\n",
    "def main():\n",
    "    \n",
    "    llm = ChatOpenAI(\n",
    "        model=\"deepseek/deepseek-chat\",\n",
    "        api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "        base_url=os.getenv(\"OPENROUTER_API_BASE\"),\n",
    "        temperature=0.7,\n",
    "        max_tokens= 500\n",
    "    )\n",
    "\n",
    "    \n",
    "    test_problem = \"Create an employee remote work policy\"\n",
    "\n",
    "    \n",
    "\n",
    "    results = {}\n",
    "\n",
    "    zero_shot_result = llm.invoke(\"test_problem\")  \n",
    "\n",
    "    results[\"zero_shot\"] = zero_shot_result.content\n",
    "    print(f\"Response length: {len(zero_shot_result.content)} characters\")\n",
    "    print(f\"Preview: {zero_shot_result.content[:100]}...\\n\")\n",
    "\n",
    "    \n",
    "\n",
    "    one_shot_template = PromptTemplate(\n",
    "        template=\"\"\"Example Policy:\n",
    "VACATION POLICY\n",
    "1. Eligibility: All full-time employees\n",
    "2. Accrual: 15 days per year\n",
    "3. Request: Submit 2 weeks in advance\n",
    "4. Approval: Manager discretion\n",
    "\n",
    "Now create: {policy_type}\"\"\",\n",
    "        input_variables=[\"policy_type\"]\n",
    "    )\n",
    "\n",
    "    \n",
    "    one_shot_prompt = one_shot_template.format(\n",
    "        policy_type=\"test_problem\"  \n",
    "    )\n",
    "    one_shot_result = llm.invoke(one_shot_prompt)\n",
    "\n",
    "    results[\"one_shot\"] = one_shot_result.content\n",
    "    print(f\"Response length: {len(one_shot_result.content)} characters\")\n",
    "    print(f\"Preview: {one_shot_result.content[:100]}...\\n\")\n",
    "\n",
    "    \n",
    "\n",
    "    examples = [\n",
    "        {\"policy\": \"sick leave\", \"format\": \"1. Coverage: 10 days/year\\n2. Documentation: Doctor's note after 3 days\"},\n",
    "        {\"policy\": \"training\", \"format\": \"1. Budget: $2000/employee/year\\n2. Approval: Required for external courses\"},\n",
    "    ]\n",
    "\n",
    "    few_shot_template = \"\"\"Examples of our policy format:\n",
    "{examples}\n",
    "\n",
    "Now create: {policy_type}\"\"\"\n",
    "\n",
    "    \n",
    "    examples_text = \"\\n\".join([f\"- {ex['policy']}: {ex['format']}\" for ex in examples])\n",
    "    few_shot_prompt = few_shot_template.format(\n",
    "        examples=examples_text,\n",
    "        policy_type=\"test_problem\" \n",
    "    )\n",
    "    few_shot_result = llm.invoke(few_shot_prompt)\n",
    "\n",
    "    results[\"few_shot\"] = few_shot_result.content\n",
    "    print(f\"Response length: {len(few_shot_result.content)} characters\")\n",
    "    print(f\"Preview: {few_shot_result.content[:100]}...\\n\")\n",
    "\n",
    "   \n",
    "    cot_template = PromptTemplate(\n",
    "        template=\"\"\"Think through this step-by-step:\n",
    "1. Consider who needs remote work\n",
    "2. Define eligibility criteria\n",
    "3. Set communication requirements\n",
    "4. Establish work hours and availability\n",
    "5. Specify equipment and security needs\n",
    "\n",
    "Problem: {problem}\n",
    "\n",
    "Work through each step to create the policy:\"\"\",\n",
    "        input_variables=[\"problem\"]\n",
    "    )\n",
    "\n",
    "    cot_prompt = cot_template.format(\n",
    "        problem=\"test_problem\"  \n",
    "    )\n",
    "    cot_result = llm.invoke(cot_prompt)\n",
    "\n",
    "    results[\"chain_of_thought\"] = cot_result.content\n",
    "    print(f\"Response length: {len(cot_result.content)} characters\")\n",
    "    print(f\"Preview: {cot_result.content[:100]}...\\n\")\n",
    "\n",
    " \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109d6536",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
